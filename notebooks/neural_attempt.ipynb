{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4bnxNFVp-59",
    "outputId": "e49b348c-a0b8-4cb5-d8bd-105ccdfa8358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (0.5.14)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\pypoetry\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install num2words transformers torch datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qpxpJI_IcyWp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from num2words import num2words #i'm not sure that it works with ru-version\n",
    "import gc\n",
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, GPT2Tokenizer\n",
    "import torch\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iaeq1MGSc0mQ"
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = r'../data'\n",
    "DATA_INPUT_PATH = r'../data/ru_with_types'\n",
    "SUBM_PATH = INPUT_PATH\n",
    "\n",
    "SUB = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n",
    "SUP = str.maketrans(\"⁰¹²³⁴⁵⁶⁷⁸⁹\", \"0123456789\")\n",
    "OTH = str.maketrans(\"፬\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dt7OcNkzc-n5",
    "outputId": "87d16323-61c6-4bcb-cfbb-324f903f27ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_train.csv:\tTotal: 10574516 Have diff value: 1322698\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = \"ru_train.csv\"\n",
    "train = open(os.path.join(INPUT_PATH, \"ru_train.csv\"), encoding='UTF8')\n",
    "line = train.readline()\n",
    "res = dict()\n",
    "total = 0\n",
    "not_same = 0\n",
    "while 1:\n",
    "    line = train.readline().strip()\n",
    "    if line == '':\n",
    "        break\n",
    "    total += 1\n",
    "    pos = line.find('\",\"')\n",
    "    text = line[pos + 2:]\n",
    "    if text[:3] == '\",\"':\n",
    "        continue\n",
    "    text = text[1:-1]\n",
    "    arr = text.split('\",\"')\n",
    "    if arr[0] != arr[1]:\n",
    "        not_same += 1\n",
    "    if arr[0] not in res:\n",
    "        res[arr[0]] = dict()\n",
    "        res[arr[0]][arr[1]] = 1\n",
    "    else:\n",
    "        if arr[1] in res[arr[0]]:\n",
    "            res[arr[0]][arr[1]] += 1\n",
    "        else:\n",
    "            res[arr[0]][arr[1]] = 1\n",
    "train.close()\n",
    "print(file + ':\\tTotal: {} Have diff value: {}'.format(total, not_same))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file output-00000-of-00100\n",
      "output-00000-of-00100:\tTotal: 13548162 Have diff value: 2325986\n",
      "reading file output-00001-of-00100\n",
      "output-00001-of-00100:\tTotal: 16529231 Have diff value: 3331025\n",
      "reading file output-00002-of-00100\n",
      "output-00002-of-00100:\tTotal: 19520932 Have diff value: 4334763\n",
      "reading file output-00003-of-00100\n",
      "output-00003-of-00100:\tTotal: 22477744 Have diff value: 5328391\n",
      "reading file output-00004-of-00100\n",
      "output-00004-of-00100:\tTotal: 25446261 Have diff value: 6328615\n",
      "reading file output-00005-of-00100\n",
      "output-00005-of-00100:\tTotal: 28421732 Have diff value: 7328628\n",
      "reading file output-00006-of-00100\n",
      "output-00006-of-00100:\tTotal: 31401788 Have diff value: 8333704\n",
      "reading file output-00007-of-00100\n",
      "output-00007-of-00100:\tTotal: 34377043 Have diff value: 9332343\n",
      "reading file output-00008-of-00100\n",
      "output-00008-of-00100:\tTotal: 37351670 Have diff value: 10334059\n",
      "reading file output-00009-of-00100\n",
      "output-00009-of-00100:\tTotal: 40318973 Have diff value: 11332092\n",
      "reading file output-00010-of-00100\n",
      "output-00010-of-00100:\tTotal: 43285644 Have diff value: 12332146\n",
      "reading file output-00011-of-00100\n",
      "output-00011-of-00100:\tTotal: 46255851 Have diff value: 13331789\n",
      "reading file output-00012-of-00100\n",
      "output-00012-of-00100:\tTotal: 49214896 Have diff value: 14326099\n",
      "reading file output-00013-of-00100\n",
      "output-00013-of-00100:\tTotal: 52187693 Have diff value: 15326870\n",
      "reading file output-00014-of-00100\n",
      "output-00014-of-00100:\tTotal: 55158985 Have diff value: 16326498\n",
      "reading file output-00015-of-00100\n",
      "output-00015-of-00100:\tTotal: 58114213 Have diff value: 17320105\n",
      "reading file output-00016-of-00100\n",
      "output-00016-of-00100:\tTotal: 61087727 Have diff value: 18323397\n",
      "reading file output-00017-of-00100\n",
      "output-00017-of-00100:\tTotal: 64058829 Have diff value: 19320417\n",
      "reading file output-00018-of-00100\n",
      "output-00018-of-00100:\tTotal: 67025743 Have diff value: 20317041\n",
      "reading file output-00019-of-00100\n",
      "output-00019-of-00100:\tTotal: 70005150 Have diff value: 21320374\n",
      "reading file output-00020-of-00100\n",
      "output-00020-of-00100:\tTotal: 72974843 Have diff value: 22318386\n",
      "reading file output-00021-of-00100\n",
      "output-00021-of-00100:\tTotal: 75946680 Have diff value: 23318533\n",
      "reading file output-00022-of-00100\n",
      "output-00022-of-00100:\tTotal: 78919346 Have diff value: 24320552\n",
      "reading file output-00023-of-00100\n",
      "output-00023-of-00100:\tTotal: 81885186 Have diff value: 25316411\n",
      "reading file output-00024-of-00100\n",
      "output-00024-of-00100:\tTotal: 84853340 Have diff value: 26316691\n",
      "reading file output-00025-of-00100\n",
      "output-00025-of-00100:\tTotal: 87820165 Have diff value: 27312815\n",
      "reading file output-00026-of-00100\n",
      "output-00026-of-00100:\tTotal: 90785565 Have diff value: 28310341\n",
      "reading file output-00027-of-00100\n",
      "output-00027-of-00100:\tTotal: 93753461 Have diff value: 29309378\n",
      "reading file output-00028-of-00100\n",
      "output-00028-of-00100:\tTotal: 96711452 Have diff value: 30306498\n",
      "reading file output-00029-of-00100\n",
      "output-00029-of-00100:\tTotal: 99695447 Have diff value: 31310646\n",
      "reading file output-00030-of-00100\n",
      "output-00030-of-00100:\tTotal: 102670202 Have diff value: 32311813\n",
      "reading file output-00031-of-00100\n",
      "output-00031-of-00100:\tTotal: 105631925 Have diff value: 33310933\n",
      "reading file output-00032-of-00100\n",
      "output-00032-of-00100:\tTotal: 108616095 Have diff value: 34315120\n",
      "reading file output-00033-of-00100\n",
      "output-00033-of-00100:\tTotal: 111577941 Have diff value: 35313788\n",
      "reading file output-00034-of-00100\n",
      "output-00034-of-00100:\tTotal: 114538194 Have diff value: 36310877\n",
      "reading file output-00035-of-00100\n",
      "output-00035-of-00100:\tTotal: 117497625 Have diff value: 37308456\n",
      "reading file output-00036-of-00100\n",
      "output-00036-of-00100:\tTotal: 120469494 Have diff value: 38310588\n",
      "reading file output-00037-of-00100\n",
      "output-00037-of-00100:\tTotal: 123441947 Have diff value: 39312474\n",
      "reading file output-00038-of-00100\n",
      "output-00038-of-00100:\tTotal: 126405807 Have diff value: 40310469\n",
      "reading file output-00039-of-00100\n",
      "output-00039-of-00100:\tTotal: 129378032 Have diff value: 41309817\n",
      "reading file output-00040-of-00100\n",
      "output-00040-of-00100:\tTotal: 132349936 Have diff value: 42312487\n",
      "reading file output-00041-of-00100\n",
      "output-00041-of-00100:\tTotal: 135340458 Have diff value: 43320308\n",
      "reading file output-00042-of-00100\n",
      "output-00042-of-00100:\tTotal: 138307814 Have diff value: 44320674\n",
      "reading file output-00043-of-00100\n",
      "output-00043-of-00100:\tTotal: 141292566 Have diff value: 45323772\n",
      "reading file output-00044-of-00100\n",
      "output-00044-of-00100:\tTotal: 144263329 Have diff value: 46323082\n",
      "reading file output-00045-of-00100\n",
      "output-00045-of-00100:\tTotal: 147234363 Have diff value: 47321886\n",
      "reading file output-00046-of-00100\n",
      "output-00046-of-00100:\tTotal: 150197676 Have diff value: 48317862\n",
      "reading file output-00047-of-00100\n",
      "output-00047-of-00100:\tTotal: 153174622 Have diff value: 49320730\n",
      "reading file output-00048-of-00100\n",
      "output-00048-of-00100:\tTotal: 156161446 Have diff value: 50320278\n",
      "reading file output-00049-of-00100\n",
      "output-00049-of-00100:\tTotal: 159115888 Have diff value: 51315577\n",
      "reading file output-00050-of-00100\n",
      "output-00050-of-00100:\tTotal: 162086099 Have diff value: 52317175\n",
      "reading file output-00051-of-00100\n",
      "output-00051-of-00100:\tTotal: 165055595 Have diff value: 53318700\n",
      "reading file output-00052-of-00100\n",
      "output-00052-of-00100:\tTotal: 168018481 Have diff value: 54316186\n",
      "reading file output-00053-of-00100\n",
      "output-00053-of-00100:\tTotal: 170979656 Have diff value: 55311696\n",
      "reading file output-00054-of-00100\n",
      "output-00054-of-00100:\tTotal: 173951320 Have diff value: 56311061\n",
      "reading file output-00055-of-00100\n",
      "output-00055-of-00100:\tTotal: 176914039 Have diff value: 57307305\n",
      "reading file output-00056-of-00100\n",
      "output-00056-of-00100:\tTotal: 179876834 Have diff value: 58305282\n",
      "reading file output-00057-of-00100\n",
      "output-00057-of-00100:\tTotal: 182848316 Have diff value: 59305192\n",
      "reading file output-00058-of-00100\n",
      "output-00058-of-00100:\tTotal: 185806264 Have diff value: 60300886\n",
      "reading file output-00059-of-00100\n",
      "output-00059-of-00100:\tTotal: 188770881 Have diff value: 61296467\n",
      "reading file output-00060-of-00100\n",
      "output-00060-of-00100:\tTotal: 191741143 Have diff value: 62297607\n",
      "reading file output-00061-of-00100\n",
      "output-00061-of-00100:\tTotal: 194717310 Have diff value: 63297526\n",
      "reading file output-00062-of-00100\n",
      "output-00062-of-00100:\tTotal: 197693270 Have diff value: 64299295\n",
      "reading file output-00063-of-00100\n",
      "output-00063-of-00100:\tTotal: 200660406 Have diff value: 65298490\n",
      "reading file output-00064-of-00100\n",
      "output-00064-of-00100:\tTotal: 203649372 Have diff value: 66302533\n",
      "reading file output-00065-of-00100\n",
      "output-00065-of-00100:\tTotal: 206614240 Have diff value: 67297118\n",
      "reading file output-00066-of-00100\n",
      "output-00066-of-00100:\tTotal: 209583332 Have diff value: 68298574\n",
      "reading file output-00067-of-00100\n",
      "output-00067-of-00100:\tTotal: 212557605 Have diff value: 69297883\n",
      "reading file output-00068-of-00100\n",
      "output-00068-of-00100:\tTotal: 215518543 Have diff value: 70295189\n",
      "reading file output-00069-of-00100\n",
      "output-00069-of-00100:\tTotal: 218495211 Have diff value: 71294246\n",
      "reading file output-00070-of-00100\n",
      "output-00070-of-00100:\tTotal: 221453563 Have diff value: 72293098\n",
      "reading file output-00071-of-00100\n",
      "output-00071-of-00100:\tTotal: 224429793 Have diff value: 73294091\n",
      "reading file output-00072-of-00100\n",
      "output-00072-of-00100:\tTotal: 227386549 Have diff value: 74288982\n",
      "reading file output-00073-of-00100\n",
      "output-00073-of-00100:\tTotal: 230357558 Have diff value: 75289896\n",
      "reading file output-00074-of-00100\n",
      "output-00074-of-00100:\tTotal: 233326774 Have diff value: 76285393\n",
      "reading file output-00075-of-00100\n",
      "output-00075-of-00100:\tTotal: 236300959 Have diff value: 77287288\n",
      "reading file output-00076-of-00100\n",
      "output-00076-of-00100:\tTotal: 239283651 Have diff value: 78291660\n",
      "reading file output-00077-of-00100\n",
      "output-00077-of-00100:\tTotal: 242245510 Have diff value: 79286278\n",
      "reading file output-00078-of-00100\n",
      "output-00078-of-00100:\tTotal: 245220821 Have diff value: 80287656\n",
      "reading file output-00079-of-00100\n",
      "output-00079-of-00100:\tTotal: 248176488 Have diff value: 81282314\n",
      "reading file output-00080-of-00100\n",
      "output-00080-of-00100:\tTotal: 251148001 Have diff value: 82281068\n",
      "reading file output-00081-of-00100\n",
      "output-00081-of-00100:\tTotal: 254109034 Have diff value: 83274193\n",
      "reading file output-00082-of-00100\n",
      "output-00082-of-00100:\tTotal: 257082629 Have diff value: 84275087\n",
      "reading file output-00083-of-00100\n",
      "output-00083-of-00100:\tTotal: 260060163 Have diff value: 85275006\n",
      "reading file output-00084-of-00100\n",
      "output-00084-of-00100:\tTotal: 263044139 Have diff value: 86279443\n",
      "reading file output-00085-of-00100\n",
      "output-00085-of-00100:\tTotal: 266004929 Have diff value: 87277957\n",
      "reading file output-00086-of-00100\n",
      "output-00086-of-00100:\tTotal: 268983809 Have diff value: 88282568\n",
      "reading file output-00087-of-00100\n",
      "output-00087-of-00100:\tTotal: 271943871 Have diff value: 89276873\n",
      "reading file output-00088-of-00100\n",
      "output-00088-of-00100:\tTotal: 274906357 Have diff value: 90273190\n",
      "reading file output-00089-of-00100\n",
      "output-00089-of-00100:\tTotal: 277869665 Have diff value: 91270334\n",
      "reading file output-00090-of-00100\n",
      "output-00090-of-00100:\tTotal: 280851575 Have diff value: 92274204\n",
      "reading file output-00091-of-00100\n",
      "output-00091-of-00100:\tTotal: 283842169 Have diff value: 93282037\n",
      "reading file output-00092-of-00100\n",
      "output-00092-of-00100:\tTotal: 286806375 Have diff value: 94278831\n",
      "reading file output-00093-of-00100\n",
      "output-00093-of-00100:\tTotal: 289778516 Have diff value: 95280469\n",
      "reading file output-00094-of-00100\n",
      "output-00094-of-00100:\tTotal: 292757659 Have diff value: 96282683\n",
      "reading file output-00095-of-00100\n",
      "output-00095-of-00100:\tTotal: 295743071 Have diff value: 97290558\n",
      "reading file output-00096-of-00100\n",
      "output-00096-of-00100:\tTotal: 298692705 Have diff value: 98286589\n",
      "reading file output-00097-of-00100\n",
      "output-00097-of-00100:\tTotal: 301672473 Have diff value: 99290509\n",
      "reading file output-00098-of-00100\n",
      "output-00098-of-00100:\tTotal: 304637929 Have diff value: 100289462\n",
      "reading file output-00099-of-00100\n",
      "output-00099-of-00100:\tTotal: 307606565 Have diff value: 101288383\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DATA_INPUT_PATH)\n",
    "corrupted_files = []\n",
    "for file in files:\n",
    "    if file.startswith(\".ipynb_checkpoints\"):\n",
    "      continue\n",
    "    print('reading file ' + file)\n",
    "    train = open(os.path.join(DATA_INPUT_PATH, file), encoding='UTF8')\n",
    "    while 1:\n",
    "      try:\n",
    "          line = train.readline().strip()\n",
    "          if line == '':\n",
    "              break\n",
    "          total += 1\n",
    "          pos = line.find('\\t')\n",
    "          text = line[pos + 1:]\n",
    "          if text[:3] == '':\n",
    "              continue\n",
    "          arr = text.split('\\t')\n",
    "          if arr[0] == '<eos>':\n",
    "              continue\n",
    "          if arr[1] != '<self>':\n",
    "              not_same += 1\n",
    "\n",
    "          if arr[1] == '<self>' or arr[1] == 'sil':\n",
    "              arr[1] = arr[0]\n",
    "\n",
    "          if arr[1] == '<self>' or arr[1] == 'sil':\n",
    "              arr[1] = arr[0]\n",
    "\n",
    "          if arr[0] not in res:\n",
    "              res[arr[0]] = dict()\n",
    "              res[arr[0]][arr[1]] = 1\n",
    "          else:\n",
    "              if arr[1] in res[arr[0]]:\n",
    "                  res[arr[0]][arr[1]] += 1\n",
    "              else:\n",
    "                  res[arr[0]][arr[1]] = 1\n",
    "      except:\n",
    "        print(file + \" corrupted\")\n",
    "        corrupted_files.append(file)\n",
    "        continue\n",
    "    train.close()\n",
    "    print(file + ':\\tTotal: {} Have diff value: {}'.format(total, not_same))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1mUi7iVWdNas"
   },
   "outputs": [],
   "source": [
    "\n",
    "sdict = {}\n",
    "sdict['km2'] = 'квадратных километров'\n",
    "sdict['km²'] = 'квадратных километров'\n",
    "sdict['km'] = 'кидлметров'\n",
    "sdict['kg'] = 'килограмм'\n",
    "sdict['lb'] = 'фунтов'\n",
    "sdict['dr'] = 'доктор'\n",
    "sdict['m²'] = 'квадратных метров'\n",
    "sdict['m2'] = 'квадратных метров'\n",
    "sdict['км2'] = 'квадратных километров'\n",
    "sdict['км²'] = 'квадратных километров'\n",
    "sdict['км'] = 'кидлметров'\n",
    "sdict['кг'] = 'килограмм'\n",
    "sdict['м²'] = 'квадратных метров'\n",
    "sdict['м2'] = 'квадратных метров'\n",
    "sdict['#'] = 'номер'\n",
    "sdict['№'] = 'номер'\n",
    "sdict['%'] = 'процент'\n",
    "sdict['久'] = 'х_trans и_trans с_trans а_trans'\n",
    "sdict['石'] = 'и_trans с_trans и_trans'\n",
    "sdict['譲'] = 'д_trans з_trans ё_trans'\n",
    "sdict['千'] = 'т_trans и_trans'\n",
    "sdict['と'] = 'т_trans о_trans'\n",
    "sdict['尋'] = 'х_trans и_trans р_trans о_trans'\n",
    "sdict['の'] = 'н_trans о_trans'\n",
    "sdict['神'] = 'к_trans а_trans м_trans и_trans'\n",
    "sdict['隠'] = 'к_trans а_trans к_trans у_trans с_trans и_trans'\n",
    "sdict['し'] = 'с_trans и_trans'\n",
    "sdict['イ'] = 'и_trans'\n",
    "sdict['メ'] = 'м_trans э_trans'\n",
    "sdict['ー'] = '-'\n",
    "sdict['ジ'] = 'д_trans з_trans и_trans'\n",
    "sdict['ア'] = 'а_trans'\n",
    "sdict['ル'] = 'р_trans у_trans'\n",
    "sdict['バ'] = 'б_trans а_trans'\n",
    "sdict['ム'] = 'м_trans у_trans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVooy--vdQUr",
    "outputId": "e927dd34-a426-4fbf-b90a-4c3d9998cee8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[232]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m line = line[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\n\u001b[32m     21\u001b[39m out.write(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m + i1 + \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m + i2 + \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mres\u001b[49m:\n\u001b[32m     23\u001b[39m     srtd = \u001b[38;5;28msorted\u001b[39m(res[line].items(), key=operator.itemgetter(\u001b[32m1\u001b[39m), reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m     out.write(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m + srtd[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "changes = 0\n",
    "out = open(os.path.join(SUBM_PATH, 'baseline_ext_ru.csv'), \"w\", encoding='UTF8')\n",
    "out.write('\"id\",\"after\"\\n')\n",
    "test = open(os.path.join(INPUT_PATH, \"ru_test_2.csv\"), encoding='UTF8')\n",
    "line = test.readline().strip()\n",
    "while 1:\n",
    "    line = test.readline().strip()\n",
    "    if line == '':\n",
    "        break\n",
    "\n",
    "    pos = line.find(',')\n",
    "    i1 = line[:pos]\n",
    "    line = line[pos + 1:]\n",
    "\n",
    "    pos = line.find(',')\n",
    "    i2 = line[:pos]\n",
    "    line = line[pos + 1:]\n",
    "\n",
    "    line = line[1:-1]\n",
    "    out.write('\"' + i1 + '_' + i2 + '\",')\n",
    "    if line in res:\n",
    "        srtd = sorted(res[line].items(), key=operator.itemgetter(1), reverse=True)\n",
    "        out.write('\"' + srtd[0][0] + '\"')\n",
    "        changes += 1\n",
    "    else:\n",
    "        if len(line) > 1:\n",
    "            val = line.split(',')\n",
    "            if len(val) == 2 and val[0].isdigit and val[1].isdigit:\n",
    "                line = ''.join(val)\n",
    "\n",
    "        if line.isdigit():\n",
    "            srtd = line.translate(SUB)\n",
    "            srtd = srtd.translate(SUP)\n",
    "            srtd = srtd.translate(OTH)\n",
    "            out.write('\"' + num2words(float(srtd)) + '\"')\n",
    "            changes += 1\n",
    "        elif len(line.split(' ')) > 1:\n",
    "            val = line.split(' ')\n",
    "            for i, v in enumerate(val):\n",
    "                if v.isdigit():\n",
    "                    srtd = v.translate(SUB)\n",
    "                    srtd = srtd.translate(SUP)\n",
    "                    srtd = srtd.translate(OTH)\n",
    "                    val[i] = num2words(float(srtd))\n",
    "                elif v in sdict:\n",
    "                    val[i] = sdict[v]\n",
    "\n",
    "            out.write('\"' + ' '.join(val) + '\"')\n",
    "            changes += 1\n",
    "        else:\n",
    "            out.write('\"' + line + '\"')\n",
    "\n",
    "    out.write('\\n')\n",
    "    total += 1\n",
    "\n",
    "print('Total: {} Changed: {}'.format(total, changes))\n",
    "test.close()\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GA_Q5tAB21g0"
   },
   "outputs": [],
   "source": [
    "def dateFixer(x):\n",
    "    return x.replace('two thousand and sixteen', 'две тысячи шестнадцатого').replace('two thousand and seventeen', 'две тысячи семнадцатого').replace('eleven', 'одиннадцатого').replace('twelve', 'двенадцатого').replace('thirteen', 'тринадцатого').replace('fourteen', 'четырнадцатого').replace('fifteen', 'пятнадцатого').replace('sixteen', 'шестнадцатого').replace('seventeen', 'семнадцатого').replace('eighteen', 'восемнадцатое').replace('nineteen', 'девятнадцатого').replace('twenty-one', 'двадцать первого').replace('twenty-two', 'двадцать второго').replace('twenty-three', 'двадцать третьего').replace('twenty-four', 'двадцать четвертого').replace('twenty-five', 'двадцать пятого').replace('twenty-six', 'двадцать шестого').replace('twenty-seven', 'двадцать седьмого').replace('twenty-eight', 'двадцать восьмого').replace('twenty-nine', 'двадцать девятого').replace('twenty', 'двадцатого').replace('thirty-one', 'тридцать первого').replace('thirty', 'тридцатого').replace('two', 'второго').replace('three', 'третьего').replace('four', 'четвертого').replace('five', 'пятого').replace('six', 'шестого').replace('seven', 'седьмого').replace('eight', 'восьмого').replace('nine', 'девятого').replace('one', 'первого').replace('ten', 'десятого')\n",
    "\n",
    "results = pd.read_csv(\"../data/baseline_ext_ru.csv\")\n",
    "newColumn = results['after'].apply(dateFixer)\n",
    "results['after'] = newColumn\n",
    "results.to_csv(os.path.join(INPUT_PATH, \"baseline_fixed_dates.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaarusTextNormalizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cpu')\n",
    "        self.re_tokens = re.compile(r\"(?:[.,!?]|[а-яА-Я]\\S*|\\d\\S*(?:\\.\\d+)?|[^а-яА-Я\\d\\s]+)\\s*\")\n",
    "        \n",
    "        self.roman_numerals = {\n",
    "            'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',\n",
    "            'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX',\n",
    "            'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXVI', 'XXVII', 'XXVIII', 'XXIX', 'XXX',\n",
    "            'XL', 'L', 'LX', 'LXX', 'LXXX', 'XC', 'C', 'CC', 'CCC', 'CD', 'D', 'DC', 'DCC', 'DCCC', 'CM', 'M'\n",
    "        }\n",
    "        \n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print(\"Загружаем модель saarus72/russian_text_normalizer...\")\n",
    "            \n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                \"saarus72/russian_text_normalizer\", \n",
    "                eos_token='</s>'\n",
    "            )\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                \"saarus72/russian_text_normalizer\"\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"Модель загружена на {self.device}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке модели: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_roman_numeral(self, text):\n",
    "        text_upper = text.upper().strip()\n",
    "        return text_upper in self.roman_numerals\n",
    "    \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(self.re_tokens, text)\n",
    "    \n",
    "\n",
    "    def strip_numbers(self, s):\n",
    "        result = []\n",
    "        for part in s.split():\n",
    "            if part.isdigit():\n",
    "                while len(part) > 3:\n",
    "                    result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                    part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "                if part:\n",
    "                    result.append(part)\n",
    "            else:\n",
    "                result.append(part)\n",
    "        return \" \".join(result)\n",
    "    \n",
    "\n",
    "    def construct_prompt(self, text):\n",
    "        result = \"<SC1>\"\n",
    "        etid = 0\n",
    "        token_to_add = \"\"\n",
    "        \n",
    "        for token in self.tokenize(text) + [\"\"]:\n",
    "            if not re.search(\"[a-zA-Z\\d]\", token):\n",
    "                if token_to_add:\n",
    "                    end_match = re.search(r\"(.+?)(\\W*)$\", token_to_add, re.M)\n",
    "                    if end_match:\n",
    "                        groups = end_match.groups()\n",
    "                        result += f\"[{self.strip_numbers(groups[0])}]<extra_id_{etid}>{groups[1]}\"\n",
    "                        etid += 1\n",
    "                        token_to_add = \"\"\n",
    "                result += token\n",
    "            else:\n",
    "                token_to_add += token\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def predict(self, text):\n",
    "        try:\n",
    "            input_ids = torch.tensor([self.tokenizer.encode(text)]).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids, \n",
    "                    max_new_tokens=50, \n",
    "                    eos_token_id=self.tokenizer.eos_token_id, \n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0][1:])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при предсказании для '{text}': {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def construct_answer(self, prompt, prediction):\n",
    "        re_prompt = re.compile(r\"\\[([^\\]]+)\\]<extra_id_(\\d+)>\")\n",
    "        re_pred = re.compile(r\"\\<extra_id_(\\d+)\\>(.+?)(?=\\<extra_id_\\d+\\>|</s>)\")\n",
    "        \n",
    "        pred_data = {}\n",
    "        for match in re.finditer(re_pred, prediction.replace(\"\\n\", \" \")):\n",
    "            pred_data[match[1]] = match[2].strip()\n",
    "        \n",
    "        while match := re.search(re_prompt, prompt):\n",
    "            replace = pred_data.get(match[2], match[1])\n",
    "            prompt = prompt[:match.span()[0]] + replace + prompt[match.span()[1]:]\n",
    "        \n",
    "        return prompt.replace(\"<SC1>\", \"\")\n",
    "    \n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Модель не загружена. Вызовите сначала load_model().\")\n",
    "        \n",
    "        try:\n",
    "            # Создаем промпт\n",
    "            prompt = self.construct_prompt(text)\n",
    "            \n",
    "            # Получаем предсказание\n",
    "            prediction = self.predict(prompt)\n",
    "            \n",
    "            # Строим финальный ответ\n",
    "            answer = self.construct_answer(prompt, prediction)\n",
    "            \n",
    "            return answer.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при нормализации '{text}': {e}\")\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск финализации результата\n",
      "Загружаем модель saarus72/russian_text_normalizer...\n",
      "Модель загружена на cpu\n",
      "Финальное улучшение rule_baseline\n",
      "Исходные данные: 989880 строк\n",
      "Rule-based результаты: 989880 строк\n",
      "Общих ID: 989880\n",
      "После фильтрации: 989880 строк\n",
      "Будет проанализировано 989880 строк\n",
      "Статистика анализа:\n",
      "  - Строк с маркерами: 62041\n",
      "  - Римских цифр: 0\n",
      "  - Строк для нейронной обработки: 989\n",
      "Нормализация с помощью Saarus модели...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Нормализация:   0%|                                                                    | 0/989 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Нормализация: 100%|██████████████████████████████████████████████████████████| 989/989 [18:39<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты сохранены в ../data/final_neural_submission.csv\n",
      "Всего улучшений: 989\n",
      "\n",
      "Примеры улучшений:\n",
      "  '1393887' -> 'миллион триста девяносто три тысячи восемьсот восемьдесят семь'\n",
      "  '65340' -> 'шестьдесят пять тысяч триста сорок'\n",
      "  '2232 с.' -> 'две тысячи двести тридцать два с.'\n",
      "  '31 мая 1971 г.' -> 'тридцать одно мая тысяча девятьсот семьдесят первого г.'\n",
      "  '9 октября 1983 год' -> 'девятое октября тысяча девятьсот восемьдесят третий год'\n",
      "  '3700129890' -> 'три миллиона семьсот тысяч сто двадцать девять тысяч восемьсот девяносто'\n",
      "  '25750460' -> 'двадцать пять миллионов семьсот пятьдесят тысяч четыреста шестьдесят'\n",
      "  '43830' -> 'сорок три тысячи восемьсот тридцать'\n",
      "  '1285402227' -> 'миллиард двести восемьдесят пять миллионов четыреста две тысячи двести двадцать семь'\n",
      "  '363 Мбит/с' -> 'триста шестьдесят три Мбит/с'\n",
      "  '22 461' -> 'двадцать две тысячи четыреста шестьдесят один'\n",
      "  '12 декабря 1802 года' -> 'Двенадцатого декабря тысяча восемьсот второго года'\n",
      "  '3315,4 км.' -> 'три тысячи триста пятнадцать целых и четыре десятых км.'\n",
      "  '21 февраля 1929' -> 'Двадцать первое февраля тысяча девятьсот двадцать девятого'\n",
      "  '5891810034' -> 'пять миллиардов восемьсот девяносто одна миллион восемьсот десять тысяч тридцать четыре'\n",
      "\n",
      " Готово. Результаты сохранены в ../data/final_neural_submission.csv\n"
     ]
    }
   ],
   "source": [
    "class My_TextNormalization_Model:\n",
    "    def __init__(self):\n",
    "        self.saarus_normalizer = SaarusTextNormalizer()\n",
    "        self.is_loaded = False\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.is_loaded = self.saarus_normalizer.load_model()\n",
    "        return self.is_loaded\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "\n",
    "        if not self.is_loaded:\n",
    "            print(\"Модель не загружена. Загружаем...\")\n",
    "            if not self.load_model():\n",
    "                print(\"Не удалось загрузить модель. Возвращаем исходный текст.\")\n",
    "                return text\n",
    "        \n",
    "        return self.saarus_normalizer.normalize_text(text)\n",
    "    \n",
    "    def load_and_align_data(self, original_data_path, rule_based_output_path):\n",
    "        try:\n",
    "            original_data = pd.read_csv(original_data_path)\n",
    "            print(f\"Исходные данные: {len(original_data)} строк\")\n",
    "            \n",
    "            original_data['composite_id'] = original_data['sentence_id'].astype(str) + '_' + original_data['token_id'].astype(str)\n",
    "            \n",
    "            rule_output = pd.read_csv(rule_based_output_path)\n",
    "            print(f\"Rule-based результаты: {len(rule_output)} строк\")\n",
    "            \n",
    "            original_ids = set(original_data['composite_id'])\n",
    "            rule_ids = set(rule_output['id'])\n",
    "            \n",
    "            common_ids = original_ids.intersection(rule_ids)\n",
    "            print(f\"Общих ID: {len(common_ids)}\")\n",
    "            \n",
    "            if len(common_ids) == 0:\n",
    "                print(\"Нет общих ID!\")\n",
    "                return None, None\n",
    "            \n",
    "            original_filtered = original_data[original_data['composite_id'].isin(common_ids)].copy()\n",
    "            rule_filtered = rule_output[rule_output['id'].isin(common_ids)].copy()\n",
    "            \n",
    "            original_filtered = original_filtered.sort_values('composite_id').reset_index(drop=True)\n",
    "            rule_filtered = rule_filtered.sort_values('id').reset_index(drop=True)\n",
    "            \n",
    "            print(f\"После фильтрации: {len(original_filtered)} строк\")\n",
    "            \n",
    "            return original_filtered, rule_filtered\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке и выравнивании данных: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def contains_markers(self, text):\n",
    "        return \"_\" in str(text)\n",
    "    \n",
    "    def needs_neural_processing(self, text):\n",
    "        text_str = str(text).strip()\n",
    "        if self.contains_markers(text_str):\n",
    "            return False\n",
    "        \n",
    "        return bool(re.search(r'\\d', text_str and r'(?![а-яА-Я])(?![a-zA-Z])') or re.search(r'thousand|hundred|fifty|million|billion', text_str))\n",
    "    \n",
    "    def postprocess_neural_result(self, original_text, neural_result):\n",
    "        original_clean = str(original_text).strip()\n",
    "        neural_clean = str(neural_result).strip()\n",
    "        \n",
    "        if re.match(r'^[а-яА-Я\\s]+$', neural_clean) and not(re.match(r'\\d', original_clean)) and not(re.match(r'[а-яА-Я]', original_clean)):\n",
    "            if self.saarus_normalizer.is_roman_numeral(original_clean):\n",
    "                return neural_clean\n",
    "                \n",
    "            letters = []\n",
    "            for char in neural_clean.replace(' ', ''):\n",
    "                if char.isalpha():\n",
    "                    letters.append(f\"{char.lower()}_trans\")\n",
    "                else:\n",
    "                    letters.append(char)\n",
    "            return ' '.join(letters)\n",
    "        return neural_clean\n",
    "    \n",
    "    def improve_rule_based_results(self, original_data_path, rule_based_output_path, output_path, sample_size=None):\n",
    "        print(\"Финальное улучшение rule_baseline\")\n",
    "        \n",
    "        if not self.is_loaded:\n",
    "            if not self.load_model():\n",
    "                print(\"Не удалось загрузить модель\")\n",
    "                return\n",
    "        \n",
    "        original_data, rule_output = self.load_and_align_data(original_data_path, rule_based_output_path)\n",
    "        \n",
    "        if original_data is None or rule_output is None:\n",
    "            print(\"Не удалось загрузить или выровнять данные\")\n",
    "            return\n",
    "        \n",
    "        if sample_size is not None and sample_size < len(rule_output):\n",
    "            indices = rule_output.sample(n=sample_size, random_state=42).index\n",
    "            rule_output = rule_output.loc[indices].reset_index(drop=True)\n",
    "            original_data = original_data.loc[indices].reset_index(drop=True)\n",
    "            print(f\"Используем выборку из {sample_size} строк\")\n",
    "        \n",
    "        print(f\"Будет проанализировано {len(rule_output)} строк\")\n",
    "        \n",
    "        texts_to_normalize = []\n",
    "        indices_to_normalize = []\n",
    "        \n",
    "        with_marker_count = 0\n",
    "        needs_neural_count = 0\n",
    "        roman_numerals_count = 0\n",
    "        \n",
    "        for i in range(len(rule_output)):\n",
    "            rule_result = rule_output.iloc[i]['after']\n",
    "            \n",
    "            if self.contains_markers(rule_result):\n",
    "                with_marker_count += 1\n",
    "            \n",
    "            if self.needs_neural_processing(rule_result):\n",
    "                if self.saarus_normalizer.is_roman_numeral(str(rule_result).strip()):\n",
    "                    roman_numerals_count += 1\n",
    "                    continue  # Пропускаем римские цифры\n",
    "                \n",
    "                needs_neural_count += 1\n",
    "                texts_to_normalize.append(original_data.iloc[i]['before'])\n",
    "                indices_to_normalize.append(i)\n",
    "        \n",
    "        print(f\"Статистика анализа:\")\n",
    "        print(f\"  - Строк с маркерами: {with_marker_count}\")\n",
    "        print(f\"  - Римских цифр: {roman_numerals_count}\")\n",
    "        print(f\"  - Строк для нейронной обработки: {needs_neural_count}\")\n",
    "        \n",
    "        if not texts_to_normalize:\n",
    "            print(\"Нет строк для нейронной обработки. Копируем исходные результаты.\")\n",
    "            rule_output.to_csv(output_path, index=False)\n",
    "            return\n",
    "        \n",
    "        normalization_cache = {}\n",
    "        \n",
    "        print(\"Нормализация с помощью Saarus модели...\")\n",
    "        for text in tqdm(texts_to_normalize, desc=\"Нормализация\"):\n",
    "            try:\n",
    "                normalized = self.normalize_text(text)\n",
    "                \n",
    "                final_result = self.postprocess_neural_result(text, normalized)\n",
    "                \n",
    "                normalization_cache[text] = final_result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при нормализации '{text}': {e}\")\n",
    "                normalization_cache[text] = str(text)\n",
    "        \n",
    "        improved_results = rule_output.copy()\n",
    "        changes_count = 0\n",
    "        \n",
    "        for i, text in zip(indices_to_normalize, texts_to_normalize):\n",
    "            if text in normalization_cache:\n",
    "                improved_text = normalization_cache[text]\n",
    "                if improved_text != str(text):\n",
    "                    improved_results.at[i, 'after'] = improved_text\n",
    "                    changes_count += 1\n",
    "        \n",
    "        improved_results.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nРезультаты сохранены в {output_path}\")\n",
    "        print(f\"Всего улучшений: {changes_count}\")\n",
    "        \n",
    "        print(\"\\nПримеры улучшений:\")\n",
    "        shown = 0\n",
    "        for original_text, improved_text in normalization_cache.items():\n",
    "            if str(original_text) != str(improved_text) and shown < 15:\n",
    "                print(f\"  '{original_text}' -> '{improved_text}'\")\n",
    "                shown += 1\n",
    "\n",
    "def run_final_pipeline(original_data_path, rule_based_output_path, output_path, sample_size=None):\n",
    "\n",
    "    print(\"Запуск финализации результата\")\n",
    "    \n",
    "    if not os.path.exists(original_data_path):\n",
    "        print(f\"Файл {original_data_path} не найден\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(rule_based_output_path):\n",
    "        print(f\"Файл {rule_based_output_path} не найден\")\n",
    "        return False\n",
    "    \n",
    "    model = My_TextNormalization_Model()\n",
    "    \n",
    "    if not model.load_model():\n",
    "        print(\"Не удалось загрузить модель\")\n",
    "        return False\n",
    "    \n",
    "    model.improve_rule_based_results(\n",
    "        original_data_path=original_data_path,\n",
    "        rule_based_output_path=rule_based_output_path,\n",
    "        output_path=output_path,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Готово. Результаты сохранены в {output_path}\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_pipeline(\n",
    "        original_data_path=\"../data/ru_test_2.csv\",\n",
    "        rule_based_output_path=\"../data/baseline_fixed_dates.csv\",\n",
    "        output_path=\"../data/final_neural_submission.csv\",\n",
    "        #sample_size=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
